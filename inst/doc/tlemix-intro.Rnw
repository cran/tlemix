\documentclass[JSS]{vignette}

\title{TLEmix: A General Framework for Robust Fitting of Finite Mixture Models in \proglang{R}}

%\subtitle{\pkg{TLEmix} version \Sexpr{packageDescription("tlemix",fields="Version")}\\\today}

\Plaintitle{TLEmix: A General Framework for Robust Fitting of Finite Mixture Models in R} 
\Shorttitle{TLEmix: Robust Fitting of Finite Mixture Models in \proglang{R}}

\author{Raja Patnaik, Alexander Eisl, Roland Boubela, and Peter Filzmoser\\Vienna University of Technology} 
\Plainauthor{Raja Patnaik}

\usepackage[utf8]{inputenc}
\usepackage{listings, amssymb, amsmath, graphicx}
\newcommand{\R}{\proglang{R}}

<<echo=false,results=hide>>=
set.seed(1504)
options(width=65)
ps.options(family="Times")
library(tlemix)
data(gaussData)
@

\Abstract{
  TLEmix implements a general framework for robustly fitting
  discrete mixtures of regression models in the \R{} statistical
  computing environment. It implements the FAST-TLE algorithm
  and uses the R package FlexMix as a computational engine for 
  fitting mixtures of general linear models (GLMs) and model-based clustering in R.
}

\Keywords{\proglang{R}, finite mixture models, model based clustering, robustness}
\Plainkeywords{R, finite mixture models, model based clustering, robustness}

\Address{
  Peter Filzmoser \\
  Department of Statistics and Probability Theory \\
  Vienna University of Technology \\
  A-1040 Vienna, Austria, Wiedner Hauptstr. 8-10 \\
  E-mail: \email{P.Filzmoser@tuwien.ac.at}\\
  URL: \url{http://statistik.tuwien.ac.at/public/filz/}
}

%%\usepackage{Sweave} %% already provided by jss.cls
%%\VignetteIndexEntry{TLEmix: A General Framework for Robust Fitting of Finite Mixture Models in R}}
%%\VignetteDepends{tlemix}
%%\VignetteKeywords{R, finite mixture models, model based clustering, robustness}
%%\VignettePackage{tlemix}

\begin{document}

\section{Introduction}
\label{sec:introduction}

The initial approach to mixture analysis was first undertaken by the biometrician Karl Pearson when he was given a data set by the famous zoologist Walter Frank Raphael Weldon in 1894. In his extensive data analysis Pearson fitted a model consisting of two normal probability density functions with different means ($\mu_1$ and $\mu_2$) and different variances ($\sigma_1^2$ and $\sigma_2^2$) in proportions $\pi_1$ and $\pi_2$ to the data. Although he did not use maximum likelihood to fit the model Pearson's estimation involving the method of moments was nearly as accurate (see \cite{MP2000}).
    
Since then finite mixture models have found a variety of applications over almost 100 years, but have experienced a significant boost in popularity during the last decade as available computing power has been growing exponentially. In particular, the 1977 published approach of the EM algorithm lead to increasing interest in finite mixture models as it tremendously simplified the maximum likelihood estimation. This paper is therefore organised as follows: First the basic concept of trimmed likelihood estimation as well as the FAST-TLE algorithm implemented by TLEmix are discussed. Section \ref{sec:using-tlemix} then demonstrates how to use TLEmix to robustly fit finite mixture models. 

% All computations and graphics in this paper have been done with
% \pkg{tlemix} version 0.0-2 and \R{} version 2.7.0 using Sweave.

\section{Trimmed Likelihood Estimator}
\label{sec:tle}

\subsection{The Weighted Trimmed Likelihood Estimator}
 Let the observations $y_1,\ldots,y_n$ denote values generated by an arbitrary probability density function $\varphi(y;\theta)$ with unknown parameter vector $\theta \in \Theta^p \subset \mathbb{R}^p$. Then, according to \cite{VN1998}, the Weighted Trimmed Likelihood Estimator (WTLE) is given by
    \begin{equation}
    WTLE(k) (y_1,\ldots,y_n) = \hat{\theta}_{WTLE} = \arg \min_{\theta \in \Theta^p} \sum_{i=1}^k w_{\nu(i)} f(y_{\nu(i)}; \theta),\label{wtle}
    \end{equation}
    where $f(y_i; \theta)$ corresponds to $- \log \varphi(y_i;\theta)$ and $f(y_{\nu(1)}; \theta) \leq f(y_{
   \nu(2)}; \theta) \leq \ldots \leq f(y_{\nu(n)}; \theta)$ are the ordered density values for a fixed $\theta$. The permutation $\nu= (\nu(1),\ldots,\nu(n))$ of the indices $1,\ldots,n$ may depend on $\theta$ as well.
   
   Depending on the nondecreasing weight function $w_{\nu(i)}$ with $w_i \geq 0$ for $i=1,\ldots,n$ (at least $w_{\nu(k)} > 0$), the WTLE can accommodate different estimators. The median likelihood estimator MedLE(k) defined by \cite{NN1990}
   \begin{equation}
   \text{MedLE}(y_1,\ldots,y_n) = \arg \min_{\theta \in \Theta^p} \underset{i}{\operatorname{med}} (- \ln \varphi(y_i; \theta)),\label{medle}
   \end{equation}
   is obtained by $w_{\nu(i)}=0$ for $i=1,\ldots,k-1,k+1,\ldots,n$ and $w_{\nu(k)} = 1$, and the TLE
   \begin{equation}
   \text{TLE}(k) (y_1,\ldots,y_n) = \arg \min_{\theta \in \Theta^p} \sum_{i=1}^k \left\lbrace - \ln(\varphi(y;\theta))_{(i)} \right\rbrace,\label{tle}
   \end{equation}
   is obtained if $w_{\nu(i)}=1$ for $i=1,\ldots,k$ and $w_{\nu(i)}=0$ otherwise.
   
   From a combinatorial point of view the WTLE yields the minima by subsampling the data:
   \begin{equation}
   \min_{\theta \in \Theta^p} \sum_{i=1}^k \left\lbrace w_{\nu(i)} f(y_{\nu(i)};\theta) \right\rbrace= \min_\tau \min_{\theta \in \Theta^p} \sum_{i=1}^k \left\lbrace w_i f(y_{\tau(i)};\theta) \right\rbrace,
   \end{equation}
   where $\tau = (\tau(1),\ldots,\tau(n))$ denotes any permutation of the indices, so that all $k$ subsets of the set $\lbrace 1,\ldots,n \rbrace$ are considered.

\subsection{The FAST-TLE Algorithm}
\label{sec:fast-tle}

Recalling the definition of the WTLE in Eq. (\ref{wtle}) it is assumed that minimisation is achieved by subsampling the data. In view of the combinatorial nature of the algorithm, computing the WTLE for large data sets can proceed very slowly. Thus, an approximative solution was developed in \cite{NM2003}.
   
   The general idea is to divide the algorithm into two separate, iterative processes. The trial step consists of drawing finitely many random subsamples of size $k^*$. In the following, for any one of those the ML estimate is computed and the subsamples of size $k$ with the lowest TL values are kept for further processing. This way in every step those sets are selected, which give an improved fit of the model. However, given, a small data set is being processed, all possible subsets with size $k$ can be considered.
   
   Continuing this iterative procedure yields guaranteed convergence of the algorithm as there are only $\binom{n}{k}$ $k$-subsets in all. To assure that there always exists a solution to the optimisation problem in Eq. (\ref{wtle}), it is assumed that the finite set $F$ is $d$-full and $k \leq d$. Note that in the normal linear regression case the FAST-TLE reduces to the FAST-LTS algorithm.
   
   As for the choice of $k$, a reasonable value would be $\lfloor (n+d+1)/2 \rfloor$ as it would maximise the BP of the TLE algorithm. However, any value between $d$ and $n$ can be chosen for $k$. The size $k^*$ of the subsamples largely depends on the fullness parameter $d$ and can be defined as $k^* = d + 1$ in order to increase the chance of drawing at least one outlier free subsample (see \cite{NM2003}).

\pagebreak[4]
\section{Using TLEmix}
\label{sec:using-tlemix}

\SweaveOpts{width=12,height=8,eps=FALSE}

As a simple example we use the artificial data set \texttt{gaussData} included in the library \texttt{tlemix} consisting of 80 observations from a mixture of two normal distributions. In order to analyse the performance of the robust FAST-TLE algorithm 20 outliers were added.

First a model framework has to be created that will be passed to the \texttt{TLE} method as the data set
<<>>= 
library(tlemix)
data(gaussData)
@

We can fit this model in \R{} using the following commands:
<<results=hide>>= 
est.tle <- TLE(y~x,family="gaussian",data=gaussData,Density=flexmix.Density,
		Estimate=flexmix.Estimate,msglvl=1,nc=2,kTrim=80,nit=10)
@
The argument \verb|kTrim=80| could be omitted, but then too many data points
would be trimmed. Moreover, the solution could be improved by increasing
the number of iterations for instance to \verb|nit=100|.
We can get a first look at the estimated parameters of mixture component~1 by 
<<>>=
parameters(est.tle@estimate, component=1)
@ 
and
<<>>=
parameters(est.tle@estimate, component=2)
@ 
for component~2. A cross-tabulation of true classes and cluster
memberships can be obtained by
<<>>=
table(gaussData$c, est.tle@tleclusters)
@ 
The summary method
<<>>=
summary(est.tle)
@ 
gives the trimming parameter, the number of observations, the number of outliers and prints the \texttt{estimate} object.

Calling the summary method for the flexmix object \texttt{est.tle@estimate}
<<>>=
summary(est.tle@estimate)
@
displays the estimated prior probabilities $\hat\pi_k$, the
number of observations assigned to the corresponding clusters, the
number of observations where $p_{nk}>\delta$ (with a default of
$\delta=10^{-4}$), and the ratio of the latter two numbers.

For this example the method \texttt{tleplot} can be used to visualise the 2-dimensional data frame. For each cluster identified by the method \texttt{TLE} a different colour is used for indication purposes. Outliers are depicted as black triangles (see Figure \ref{fig:normal}).
<<results=hide>>=
tleplot(est.tle, gaussData)
@

\begin{figure}[htbp]
  \centering
<<fig=true, echo=false, results=hide, height=6, width=6>>=
tleplot(est.tle, gaussData)
@ 
  \caption{Mixture of two normal components with noise. Scatterplot 
  with cluster memberships.}
  \label{fig:normal}
\end{figure}

Additionally, the flexmix object can be plotted by
<<results=hide>>=
plot(est.tle@estimate)
@
and will yield rootograms of the posterior class probabilities 
to visually assess the cluster structure (see Figure \ref{fig:rootogram}).
\begin{figure}[htbp]
  \centering
<<fig=true, echo=false, results=hide, height=6>>=
print(plot(est.tle@estimate))
@ 
  \caption{Rootogram of the posterior class probabilites.}
  \label{fig:rootogram}
\end{figure}

Usually in each component a lot of observations have posteriors close
to zero, resulting in a high count for the corresponding bin in the
rootogram which obscures the information in the other bins. To avoid
this problem, all probabilities with a posterior below a threshold are
ignored (we again use $10^{-4}$).  A peak at probability 1 indicates that a
mixture component is well seperated from the other components, while
no peak at 1 and/or significant mass in the middle of the unit
interval indicates overlap with other components.

\pagebreak[4]

\bibliography{references}

\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
